{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Gradient Descent:Parameters are updated after computing the gradient of error with respect to the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = [30, 35, 37, 59, 70, 76, 88, 100 ]\n",
    "y = [1100, 1423, 1377, 1800, 2304, 2588, 3495, 4839]\n",
    "\n",
    "X = np.reshape(X,(-1,1))\n",
    "y = np.array(y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = len(y) # number of examples : 8\n",
    "learningrate = 0.1\n",
    "\n",
    "ones = np.ones((m,1)) \n",
    "X = np.hstack((ones, X)) # shape : (8,2)\n",
    "y = y[:,np.newaxis] # shape : (8.1)\n",
    "\n",
    "theta = np.zeros(( X.shape[1],1)) # shape : (2,1)\n",
    "\n",
    "loss = []\n",
    "iteration = [] # epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    iteration.append(i)\n",
    "    y_predicted = np.dot(X,theta)\n",
    "\n",
    "    error = y - y_predicted # error\n",
    "    sq_error = error* error # square the error\n",
    "\n",
    "    cost = sq_error.sum() / (2*m) # compute cost\n",
    "\n",
    "    loss.append(cost) # append the cost\n",
    "\n",
    "    #iterations.append(i)\n",
    "\n",
    "    # Compute Gradient Descent\n",
    "    temp = y_predicted - y\n",
    "    temp = np.dot(X.T, temp)\n",
    "\n",
    "    theta = theta - (learningrate/m) * temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(iteration,loss)\n",
    "plt.grid()\n",
    "plt.title(\"Batch Gradient Descent\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "%%pixie_debugger\n",
    "import random\n",
    "def find_max (values):\n",
    "    max = 0\n",
    "    for val in values:\n",
    "        if val > max:\n",
    "            max = val\n",
    "            \n",
    "    return max\n",
    "find_max(random.sample(range(100), 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
