{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Loss : In machine learning, Loss function is used to find error or deviation in the learning process.\n",
    "\n",
    "List of Loss Functions:\n",
    "\n",
    "     1. mean_squared_error\n",
    "     2. mean_absolute_error\n",
    "     3. mean_absolute_percentage_error\n",
    "     4. mean_squared_logarithmic_error\n",
    "     5. squared_hinge\n",
    "     6. hinge\n",
    "     7. categorical_hinge\n",
    "     8. logcosh\n",
    "     9. huber_loss\n",
    "     10. categorical_crossentropy\n",
    "     11. sparse_categorical_crossentropy\n",
    "     12. binary_crossentropy\n",
    "     13. kullback_leibler_divergence\n",
    "     14. poisson\n",
    "     15. cosine_proximity\n",
    "     16. is_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Regression losses\n",
    "    - Mean Squared error\n",
    "    - Mean Absolute Error\n",
    "    - Mean Absolute percentage error\n",
    "    - Mean Squared Logarithmic Error\n",
    "    - Cosine Similarity\n",
    "    - Huber\n",
    "    - Log Cosh\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Probabilistic Losses\n",
    "    - Binary Crossentropy (BCE)\n",
    "    - Categorical Crossentropy or SoftMax Loss\n",
    "    - Sparse Categorical Crossentropy\n",
    "    - Poisson\n",
    "    - Kullback-Leibler Divergence\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hinge losses for “maximum-margin” classification\n",
    "    - Hinge\n",
    "    - Squared Hinge\n",
    "    - Categorical Hinge\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hinge\n",
    "loss = maximum(1 - y_true * y_pred, 0)\n",
    "\n",
    "\n",
    "y_true values are expected to be -1 or 1\n",
    "If binary (0 or 1) labels are provided they will be converted to -1 or 1.\n",
    "\n",
    "np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SquaredHinge class\n",
    "\n",
    "loss = square(maximum(1 - y_true * y_pred, 0))\n",
    "\n",
    "y_true values are expected to be -1 or 1. If binary (0 or 1) labels are provided we will convert them to -1 or 1\n",
    "\n",
    "np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CategoricalHinge class\n",
    "\n",
    "loss = maximum(neg - pos + 1, 0) where neg=maximum((1-y_true)*y_pred) and pos=sum(y_true*y_pred)\n",
    "\n",
    "pos = np.sum(y_true * y_pred, axis=-1)\n",
    "neg = np.amax((1. - y_true) * y_pred, axis=-1)\n",
    "loss = np.maximum(0., neg - pos + 1.)\n",
    "    \n",
    "y_true values are expected to be 0 or 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Mean Squared Error : MSE\n",
    "\n",
    "MSE = np.square(np.subtract(Y_true,Y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Mean Squared Logarithmic Error : (MSLE)\n",
    "\n",
    "MSE = np.square(np.subtract(np.log(Y_true +1),np.log(Y_pred +1))).mean()\n",
    "\n",
    "Find the relative difference between the true and the predicted value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Mean Absolute Error :MAE\n",
    "\n",
    "MAE = np.abs(np.subtract(Y_true,Y_pred)).mean()\n",
    "\n",
    "Cosider only the magniture (length : sqrt.(x2 + y2) )\n",
    "does not conisder direction (Negative or Positisve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Mean Absolute percentage error :MAPE\n",
    "\n",
    "MAPE = (np.abs(np.subtract(Y_true,Y_pred)) / Y_true).mean() * 100\n",
    "\n",
    "Cosider only the magniture (length : sqrt.(x2 + y2) )\n",
    "does not conisder direction (Negative or Positisve)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Cosine Similarity\n",
    "Measure of similarity between two non-zero vectors of an inner product space\n",
    "cosine similarity (CS) = (A . B) / (||A|| ||B||)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Huber :(MSE + MAE) based on delta value we chosse either MSE or MAE\n",
    "\n",
    "This function is quadratic(MSE) for small values of a and linear(MAE) for large values, \n",
    "\n",
    "loss = 0.5 * x^2                  if |x| <= d\n",
    "loss = 0.5 * d^2 + d * (|x| - d)  if |x| > d\n",
    "\n",
    "where d is delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Log Cosh : Computes the logarithm of the hyperbolic cosine of the prediction error\n",
    "Log-cosh is the logarithm of the hyperbolic cosine of the prediction error.\n",
    "\n",
    "logcosh = log((exp(x) + exp(-x))/2)\n",
    "\n",
    "x is the error y_pred - y_true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Squared Hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Categorical Hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Binary Cross Entropy : \n",
    "    - used where two class present in the traget varible\n",
    "    - By default, the sum_over_batch_size reduction is used.\n",
    "    - This means that the loss will return the average of the per-sample losses in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is a good cross-entropy score?\n",
    "\n",
    "'''\n",
    "Cross-Entropy = 0.00: Perfect probabilities.\n",
    "Cross-Entropy < 0.02: Great probabilities.\n",
    "Cross-Entropy < 0.05: On the right track.\n",
    "Cross-Entropy < 0.20: Fine.\n",
    "Cross-Entropy > 0.30: Not great.\n",
    "Cross-Entropy > 1.00: Terrible.\n",
    "Cross-Entropy > 2.00 Something is broken.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Log Loss : “log loss“,or  “cross-entropy” or “negative log-likelihood” a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Categorical Cross Entropy\n",
    "    - It compares the predicted probability distribution with target probability distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sparse Categorical Crossentropy\n",
    "\n",
    "    if you use categorical_crossentropy you use one hot encoding, \n",
    "    if you use sparse_categorical_crossentropy you encode as normal integers. \n",
    "\n",
    "  When to use Categorical Cross Entropy\n",
    "\n",
    "  If your targets are one-hot encoded, use categorical_crossentropy. Examples of one-hot encodings:\n",
    "\n",
    "  sIf your targets are integers, use sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
