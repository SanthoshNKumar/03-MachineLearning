{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Data Scaling\n",
    "    1.Standardziation\n",
    "    2.MinMaxScaler\n",
    "    3.Normalization\n",
    "        - L1 Norm\n",
    "        - L2 Norm\n",
    "    4.Z_scores\n",
    "    5.Robust Scaling\n",
    "\n",
    "2. Categorical Encoding\n",
    "    1.Number Encoder\n",
    "       - label Encoding\n",
    "       - Ordinal Encoding\n",
    "    2.Binarization\n",
    "    3.Label Binarization\n",
    "    4.One Hot Encoding\n",
    "\n",
    "3. Create Dummy dataset\n",
    "    1.Make Blobs\n",
    "    2.Make Circles\n",
    "    3.Make Classification\n",
    "    4.Make Moons\n",
    "    5.Make Regression\n",
    "    6.Make Data points on 4 Quarants\n",
    "\n",
    "4. Outlier Detection\n",
    "    1.Density And Boxplot\n",
    "    2.Hypothesis Testing_One_Sampled_T-test\n",
    "    3.Inter Quartile Range(IQR)\n",
    "    4.IsolationForest\n",
    "    5.K-Means Clustering for Outlier Detection\n",
    "    6.RANSAC (Random sample consensus)\n",
    "    7.Scatter Plot\n",
    "    8.Standard Deviation Method\n",
    "    9.Statistics_Gaussin_Extreme values\n",
    "    10.Statistics_Gaussin_Log Tails\n",
    "    11.Z-scores\n",
    "\n",
    "5. Get Gaussian Distribution\n",
    "    1.Add More Samples\n",
    "    2.Identify and Remove the Extreme Values in the data distribution\n",
    "    3.Handle Long Tiles (simple threshold values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Handling Missing Values\n",
    "    1. Assign unique value using 'fillna'\n",
    "    2. bfill and ffill\n",
    "    3. Delete rows\n",
    "    4. Fill missing values imputers\n",
    "    5. fill mean value\n",
    "    6. Predicting the missing values\n",
    "    7. Visualize the missing values on graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1.Understanding and get insight into the data\n",
    "1.Univariate Analysis \n",
    "    - Central tendency\n",
    "    - Spread\n",
    "    - Skewness and Kurtosis\n",
    "    - Bar Cart\n",
    "    - Histogram\n",
    "    - \n",
    "    \n",
    "2.Bivariate Analysis\n",
    "    - Scatterplot\n",
    "    - \n",
    "3.Multivariate Analysis\n",
    "    - Grouped Bar Chart\n",
    "    - Heat map\n",
    "3. Correlation Analysis\n",
    "4.Identifyiing Outliers\n",
    "6.Pair Plot\n",
    "7.Distributions of the variables/features\n",
    "8.Correlation amoung the features\n",
    "9.Relationship between input features and target variables\n",
    "10.Descriptive Statistics\n",
    "11.Box Plot for feature summary\n",
    "    - Minimum\n",
    "    - First quartile\n",
    "    - Median\n",
    "    - Third quartile\n",
    "    - Maximum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Techniques\n",
    "\n",
    "    1. Chi-Square Test\n",
    "    2. Correlation Matrix\n",
    "    3. One Sample t Test\n",
    "    4. Feature Importance using Decision Trees\n",
    "    5. L1 Regularization(Lassco)\n",
    "    6. Recursive Feature Elimination(RFE)\n",
    "    7. Relative Feature Importance\n",
    "    8. Remove Correlated Features\n",
    "    9. Select K Best\n",
    "    10.T Statistics and P value\n",
    "    11.Using PCA\n",
    "    12.Variance Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Regression losses\n",
    "    1.Mean Squared error\n",
    "    2.Mean Absolute Error\n",
    "    3.Mean Absolute percentage error\n",
    "    4.Mean Squared Logarithmic Error\n",
    "    5.Cosine Similarity\n",
    "    6.Huber\n",
    "    7.Log Cosh\n",
    "\n",
    "2. Probabilistic Losses\n",
    "    1.Binary Crossentropy (BCE)\n",
    "    2.Categorical Crossentropy or SoftMax Loss\n",
    "    3.Sparse Categorical Crossentropy\n",
    "    4.Poisson\n",
    "    5.Kullback-Leibler Divergence\n",
    "\n",
    "3. Hinge losses for “maximum-margin” classification\n",
    "    1.Hinge\n",
    "    2.Squared Hinge\n",
    "    3.Categorical Hinge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1.List of Optimizers\n",
    "\n",
    "    1. Batch Gradient Descent\n",
    "    2. Stochastic Gradient Descent\n",
    "    3. Mini Batch Gradient Descent\n",
    "    4. Momentum\n",
    "    5. NAG\n",
    "    6. AdaGrad\n",
    "    7. AdaDelta and RMSProp\n",
    "    8. Adam\n",
    "    9. AdaMax\n",
    "    10.Ftrl\n",
    "\n",
    "2.List of parameters for Optimizations in Keras\n",
    "\n",
    "1. SGD\n",
    "    - Learning rate\n",
    "    - momentum\n",
    "    - nesterov = True/False\n",
    "    - name\n",
    "    - decay\n",
    "    - clipnorm :True/False\n",
    "    - clipvalue\n",
    "\n",
    "2. Adgrad\n",
    "    - learning_rate\n",
    "    - initial_accumulator_value\n",
    "    - epsilon\n",
    "    - name\n",
    "    - decay\n",
    "    - clipnorm\n",
    "    - clipvalue\n",
    "\n",
    "3.Adadelta\n",
    "    - learning_rate\n",
    "    - rho\n",
    "    - epsilon\n",
    "    - name\n",
    "    - decay\n",
    "    - clipnorm = rue/False\n",
    "    - clipvalue\n",
    "\n",
    "4. RMSProp\n",
    "    - learning_rate\n",
    "    - rho\n",
    "    - momentum\n",
    "    - epsilon\n",
    "    - centered = True/False\n",
    "    - name\n",
    "    - clipnorm = rue/False\n",
    "    - clipvalue\n",
    "\n",
    "5.Adam\n",
    "    - learning_rate\n",
    "    - beta_1\n",
    "    - beta_2\n",
    "    - epsilon\n",
    "    - amsgrad\n",
    "    - name\n",
    "    - decay\n",
    "    - clipnorm = True/False\n",
    "    - clipvalue\n",
    "\n",
    "6. AdaMax\n",
    "    - learning_rate\n",
    "    - beta_1\n",
    "    - beta_2\n",
    "    - epsilon\n",
    "    - name\n",
    "    - decay\n",
    "    - clipnorm = True/False\n",
    "    - clipvalue\n",
    "\n",
    "7. Nadam\n",
    "    - learning_rate\n",
    "    - beta_1\n",
    "    - beta_2\n",
    "    - epsilon\n",
    "    - name\n",
    "    - decay\n",
    "    - clipnorm = True/False\n",
    "    - clipvalue\n",
    "\n",
    "8. Ftrl\n",
    "    - learning_rate\n",
    "    - learning_rate_power\n",
    "    - initial_accumulator_value\n",
    "    - l1_regularization_strength\n",
    "    - l2_regularization_strength\n",
    "    - name\n",
    "    - l2_shrinkage_regularization_strength\n",
    "    - decay\n",
    "    - clipnorm = True/False\n",
    "    - clipvalue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Regression\n",
    "    1.Mean Squared Error : MSE\n",
    "    2.Root Mean Squared Error : RMSE\n",
    "    3.Mean Absolute Error : MAE\n",
    "    4.Mean Absolute Percentage Error :\n",
    "    5.Mean Squared Logorithmic Error Metrics\n",
    "    6.R-squared : (coefficient of determination)\n",
    "    7.Cosine Similarity :\n",
    "    8.LogCosh error :\n",
    "\n",
    "2. Classification\n",
    "    1.Accuracy metrics\n",
    "        - Accuracy\n",
    "        - Binary Accuracy\n",
    "        - Categorical Accuracy\n",
    "        - Top K Categorical Accuracy\n",
    "        - Sparse Top K Categorical Accuracy\n",
    "\n",
    "    2.Probablistic Metrics\n",
    "        - Log Loss /Binary Cross Entropy\n",
    "        - Categorical Cross Entropy\n",
    "        - Sparse Categorical Cross Entropy\n",
    "        - KL Divergence \n",
    "        - Poission\n",
    "\n",
    "    3.Classfication Metrics Bases on True/False Positive and Negative\n",
    "        - AUC : Application Under Curve\n",
    "        - Precision Class\n",
    "        - Recall\n",
    "        - True Positive\n",
    "        - True Negative\n",
    "        - False Positive\n",
    "        - False Negative\n",
    "        - Precision At Recall\n",
    "        - Sensitivity at Specificity\n",
    "        - Specificity at Sensitivity\n",
    "\n",
    "    4.Hinge metrics for \"maximum-margin\" classification\n",
    "        - Hinge\n",
    "        - Squared Hinge\n",
    "        - Categorical Hinge\n",
    "\n",
    "    5.Image segmentation met\n",
    "    \n",
    "    6.Accuracy : \n",
    "        - Percentage of Correct Prediction\n",
    "        - Accuracy =  (TP + TN) / (TP + TN + FP + FN)\n",
    "        - ratio of correctly predicted to the total number of points\n",
    "        - Accuracy not a reliable performance metric for imbalanced data\n",
    "  \n",
    "    7.Log Loss / Binary Crossentropy\n",
    "        - Logarithmic loss (or log loss) measures the performance of a classification model where the \n",
    "          prediction is a probability value between 0 and 1.\n",
    "        - Log loss increases as the predicted probability diverge from the actual label.\n",
    "        - Lower the log-loss value, better are the predictions of the model\n",
    "\n",
    "    8. ROC/ AUC Curve :(Receiver Operating Characteristic)\n",
    "\n",
    "5. Clustering\n",
    "    1.Calinski_Harabaz_Score\n",
    "    2.CompletenessScore\n",
    "    3.CompletenessScore_AdjustedRandScore\n",
    "    4.Homogeneity\n",
    "    5.OptimalClusters_by_Inertia\n",
    "    6.Silhouette_Score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1.Linear Regression\n",
    "    1.Linear Regression\n",
    "    2.Polynomial Regression\n",
    "    3.Ridge(L2) Regression \n",
    "    4.Lasso(L1) Regression\n",
    "    5.Robust Regression\n",
    "    6.Isotonic Regression\n",
    "    7.Logarithmic Regression\n",
    "    8.Exponential Regression\n",
    "    9.Sinusoidal_Regression\n",
    "\n",
    "2.Logistic Regression\n",
    "\n",
    "3.Decision Trees \n",
    "\n",
    "4.Naive Bayes\n",
    "\n",
    "5.SVM\n",
    "\n",
    "6.KNN\n",
    "\n",
    "7.Ensemble Learning\n",
    "\n",
    "8.Clustering\n",
    "\n",
    "9.Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. L1 Regularization (Lassco)\n",
    "2. L2 Regularization (Ridge)\n",
    "3. L1 and L2 Regularization (ElasticNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1.Resampling\n",
    "2.K Fold\n",
    "3.cross_val_score\n",
    "4.stratified sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Hyperparameter Tuning Mechanism\n",
    "    1.Grid Search CV\n",
    "    2.Random Search CV\n",
    "    3.Hyperband\n",
    "    4.Bayesian Optimization with Gaussian Process (BO-GP)\n",
    "    \n",
    "2. List of Hyperparameter in Machine Learning Algorithms\n",
    "\n",
    "    1. Linear Regression\n",
    "    \n",
    "    1.1 Ridge Regression\n",
    "        - alpha : [1,0.1,0.01,0.001,0.0001,0]\n",
    "        \n",
    "    1.2 Lassco Regression\n",
    "    \n",
    "    \n",
    "    2. Logistic Regression\n",
    "    \n",
    "    3. Decision Trees \n",
    "        - 'n_estimators' : [10, 100]\n",
    "        - 'criterion': ['gini','entropy'],\n",
    "        - 'max_features' : ['auto','log2',None],\n",
    "        - 'min_samples_split' : [2,10,25,100,200],\n",
    "        - 'min_samples_leaf':[1,11]\n",
    "        - 'max_depth' : [5,10,15,None]\n",
    "        - 'splitter' : ['best','random']\n",
    "    \n",
    "    4. Naive Bayes\n",
    "    \n",
    "    5. SVM\n",
    "        - 'C': [1,10, 100]\n",
    "        - 'kernel':['linear','poly','rbf','sigmoid']\n",
    "    \n",
    "    6. KNN\n",
    "        - 'n_neighbors' = [1,2,3,4,6,7,8]\n",
    "    \n",
    "    7. Ensemble Learning\n",
    "    \n",
    "        1. Gradiest Boosting \n",
    "            - 'n_estimators'\n",
    "            - 'max_depth'\n",
    "            - 'learning_rate'\n",
    "    \n",
    "    8. Clustering\n",
    "    \n",
    "    9. Dimensionality Reduction\n",
    "    \n",
    "    10. ANN\n",
    "        - 'optimizer': ['adam','rmsprop','sgd']\n",
    "        - 'activation': ['relu','tanh']\n",
    "        - 'batch_size': [16, 64, 16]\n",
    "        - 'neurons': range(10,100)\n",
    "        - 'epochs': [20, 50, 10]\n",
    "        - 'patience': range(3,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1.  Create pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Heroku\n",
    "2. Azure\n",
    "3. AWS EC2\n",
    "4. Docker\n",
    "5. Embedded and Android Device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
