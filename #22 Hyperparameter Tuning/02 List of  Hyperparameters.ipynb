{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Linear Regression\n",
    "  - fit_intercept\n",
    "  - normalize\n",
    "  - copy_x\n",
    "  - n_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ridge:\n",
    "  - alpha\n",
    "  - fit_intercept\n",
    "  - normalize\n",
    "  - copy_x\n",
    "  - max_iter\n",
    "  - tol\n",
    "  - solver\n",
    "  - random_state\n",
    "  \n",
    "  \n",
    "fit_intercept : Mathematically, it adds a column of 1s to the inputs.\n",
    "\n",
    "normalize:the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm\n",
    "\n",
    "max_iter : Maximum number of iterations taken for the solvers to converge.\n",
    "\n",
    "Verbose : Controls verbosity\n",
    "\n",
    "verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch.\n",
    "verbose = 1, which includes both progress bar and one line per epoch\n",
    "verbose = 0, means silent\n",
    "verbose = 2, one line per epoch i.e. epoch no./total no. of epochs\n",
    "\n",
    "n_jobs : The number of jobs to use for the computation. -1 means using all processors.\n",
    "\n",
    "tol(tolerance) : \n",
    "\n",
    "solver : Solving logistic regression is an optimization problem. (default=’liblinear’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SCIKIT_ships five different solvers.  Each solver tries to find the parameter weights that minimize a cost function\n",
    "\n",
    "  - newton-cg =  It's slow for large datasets, because it computes the second derivatives.\n",
    "  - lbfgs = Limited-memory Broyden–Fletcher–Goldfarb–Shanno. It approximates the second derivative matrix updates with gradient \n",
    "            evaluations.\n",
    "            It stores only the last few updates, so it saves memory. It isn't super fast with large data sets\n",
    "  - Liblinear = is fast with small datasets, but has problems with saddle points and can't be parallelized over multiple \n",
    "            processor cores.\n",
    "  - sag(Stochastic Average Gradient descent):Fast for big datasets.\n",
    "  - saga : Extension of sag that also allows for L1 regularization.(generally train faster than sag.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Penalty \n",
    "    - It is used to specify the method of penalization of the coefficients of noncontributing variables.\n",
    "\n",
    "C(Penalty Strength): \n",
    "    - C is the inverse of the regularization term (1/lambda). It tells the model how much large parameters are penalized, \n",
    "      smaller values result in larger penalization; must be a positive float.\n",
    "\n",
    "class_weight\n",
    "    - allows you to place greater emphasis on a class. For example, if the distribution between class 1 and class 2 is \n",
    "      heavily imbalanced, the model can treat the two distributions appropriately.\n",
    "\n",
    "max_depth\n",
    "    - Set the max depth of the tree.\n",
    "\n",
    "min_samples_split\n",
    "    - The minimum number of samples needed before a split (differentiation) is made in an internal node\n",
    "\n",
    "min_samples_leaf\n",
    "    - The minimum number of samples needed to create a leaf (decision) node.\n",
    "\n",
    "max_features\n",
    "    - Set the number of features to consider for the best node split\n",
    "\n",
    "KNN : (p value)(power parameter) : When p = 1, this is equivalent to using manhattan_distance, and euclidean_distance for p = 2.\n",
    "\n",
    "criterion(Decision Tree) is the function to measure the quality of a split\n",
    "\n",
    "alpha(Lascoo Regression) : regularization parameter.\n",
    "\n",
    "kernels: \n",
    "    - Control the manner in which the input variables will be projected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Decision Trees :\n",
    "  - Criterion : (Default : Gini) ['Gine' or 'entropy']\n",
    "  \n",
    "  - Splitter : (Default : 'best') ['best' or 'random']\n",
    "  \n",
    "  - max_depth : (Default: None) ['None' or 'Int'] : Maximum number of children nodes that can grow from the decision tree.\n",
    "    - If it is set to 3 ,then the tree will use three childern nodes and cut the tree off before it can grow any more.\n",
    "  \n",
    "  - min_sample_split :(Default : 2) : Minimum number of samples required to split an internal node\n",
    "  \n",
    "  - min_sample_leaf : (Default: 1) :Minimum number of samples or data points that are required to be at a leaf node,\n",
    "  \n",
    "  - min_weight_fraction_leaf (Default:0) : \n",
    "  \n",
    "  - max_features : (Default : None) : The number of features to be consider when looking for best split\n",
    "         - int : consider max_features features at each split.\n",
    "         - float : int(max_features * n_features)\n",
    "         - auto : sqrt(n_features)\n",
    "         - sqrt : sqrt(n_features)\n",
    "         - log2 : log2(n_features)\n",
    "         - None : n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
