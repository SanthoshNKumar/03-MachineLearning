{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "XG Boost (Extreme Gradient Boosting)\n",
    "    - It is an Extention of Gradient Boosting\n",
    "    - XGBoost is generally over 10 times faster than a gradient boosting machine. \n",
    "    - XG Boost supports multiple language Support\n",
    "    - It can automatically do parallel computation on Windows and Linux, with openmp\n",
    "    - It is Integrable with various system (Like Cloud)\n",
    "    - It famous because speed of processing and performance\n",
    "    - It is implemented top of Gradient Boosting and its advancement of Gradient Boosting\n",
    "    - This framework takes several types of input data including local data files.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "What makes XG boost run very fast?\n",
    "    - Parallelization: If you are running on single processor it will use all cores of the CPU and if you are running \n",
    "                       on distributed mode it will ensure it will use maximum available computational power \n",
    "                       available of distributed system.\n",
    "                       \n",
    "    - Cache Optimization: XG boost keeps all its intermidiate calculation or statistics in cache in such a way that whenever \n",
    "                          you try to get the information it will be fast.\n",
    "                          \n",
    "    - Out of Memory Computation: Lets say we have data which size is bigger than size of memory. in XG boost there is possible \n",
    "                         way of optimizing memory in such a way that it can work on the data which is larget than the RAM.\n",
    "                         \n",
    "    - Regularization paramerers in XG boost which avoids overfitting which were not there in normal Gradient Boosting.\n",
    "      in XG Boost Automatic regularization will happen and makes sure model will not overfitting\n",
    "      \n",
    "    - Auto pruning: in XG boost it will not allow tree to grow after certain level becasue of this model will be \n",
    "      effective and robust. this will happen internall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost Classifers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.02%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# load data\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = pd.read_csv(\"pima-indians-diabetes.csv\",names= names)\n",
    "array = data.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "\n",
    "# split data into train and test sets\n",
    "seed = 7\n",
    "test_size = 0.33\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions for test data\n",
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XG Boost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:06:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.0.0/src/objective/regression_obj.cu:167: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "RMSE: 6.455243\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "boston = load_boston()\n",
    "data = pd.DataFrame(boston.data)\n",
    "\n",
    "X, y = data.iloc[:,:-1],data.iloc[:,-1]\n",
    "\n",
    "data_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "data_dmatrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, \n",
    "                          learning_rate = 0.1,max_depth = 5, \n",
    "                          alpha = 10, n_estimators = 10)\n",
    "\n",
    "xg_reg.fit(X_train,y_train)\n",
    "\n",
    "preds = xg_reg.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
