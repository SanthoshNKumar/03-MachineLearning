{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is one of the many machine learning algorithms. A decision tree is a decision tool. Its similar to a tree-like model in computer science. (root at the top, leaves downwards)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have couple of other algorithms there, so why do we have to choose Decision trees??\n",
    "\n",
    "well, there might be many reasons but I believe a few which are\n",
    "\n",
    "1.Decision tress often mimic the human level thinking so its so simple to understand the data and make some good interpretations.\n",
    "\n",
    "\n",
    "2.Decision trees actually make you see the logic for the data to interpret(not like black box algorithms like SVM,NN,etc..)\n",
    "\n",
    "\n",
    "For example : if we are classifying bank loan application for a customer, the decision tree may look like this\n",
    "\n",
    "<img src=\"img/1..jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the logic how it is making the decision.\n",
    "\n",
    "It’s simple and clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So what is the decision tree??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree is a tree where each node represents a feature(attribute), each link(branch) represents a decision(rule) and each leaf represents an outcome(categorical or continues value).\n",
    "\n",
    "\n",
    "The whole idea is to create a tree like this for the entire data and process a single outcome at every leaf(or minimize the error in every leaf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Okay so how to build this??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are couple of algorithms there to build a decision tree , we only talk about a few which are\n",
    "\n",
    "1.CART (Classification and Regression Trees) → uses Gini Index(Classification) as metric.\n",
    "\n",
    "2.ID3 (Iterative Dichotomiser 3) → uses Entropy function and Information gain as metrics.\n",
    "\n",
    "Lets just first build decision tree for classification problem using above algorithms,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with using the ID3 algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s just take a famous dataset in the machine learning world which is whether dataset(playing game Y or N based on whether condition).\n",
    "\n",
    "We have four X values (outlook,temp,humidity and windy) being categorical and one y value (play Y or N) also being categorical.\n",
    "\n",
    "so we need to learn the mapping (what machine learning always does) between X and y.\n",
    "\n",
    "<img src=\"img/2.jpg\">\n",
    "\n",
    "\n",
    "\n",
    "This is a binary classification problem, lets build the tree using the ID3 algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a tree, we need to have a root node first and we know that nodes are features/attributes(outlook,temp,humidity and windy),\n",
    "\n",
    "<b>so which one do we need to pick first??</b>\n",
    "\n",
    "\n",
    "Answer: determine the attribute that best classifies the training data; use this attribute at the root of the tree. Repeat this process at for each branch.\n",
    "\n",
    "This means we are performing top-down, greedy search through the space of possible decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "okay so how do we choose the best attribute?\n",
    "\n",
    "Answer: use the attribute with the highest information gain in ID3\n",
    "\n",
    "In order to define information gain precisely, we begin by defining a measure commonly used in information theory, called entropy that characterizes the (im)purity of an arbitrary collection of examples.”\n",
    "\n",
    "<img src=\"img/3.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a binary classification problem\n",
    "\n",
    "1.If all examples are positive or all are negative then entropy will be zero i.e, low.\n",
    "\n",
    "\n",
    "2.If half of the examples are of positive class and half are of negative class then entropy is one i.e, high.\n",
    "\n",
    "<img src=\"img/4.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay lets apply these metrics to our dataset to split the data(getting the root node)\n",
    "\n",
    "Steps"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.compute the entropy for data-set\n",
    "2.for every attribute/feature:\n",
    "       1.calculate entropy for all categorical values2.take average information entropy for the current attribute3.calculate gain for the current attribute\n",
    "3. pick the highest gain attribute.\n",
    "4. Repeat until we get the tree we desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the heck???\n",
    "\n",
    "Okay I got it , if it does not make sense to you , let me make it sense to you.\n",
    "\n",
    "Compute the entropy for the weather data set:\n",
    "\n",
    "<img src=\"img/5.jpg\">\n",
    "\n",
    "For every feature calculate the entropy and information gain\n",
    "\n",
    "<img src=\"img/6.jpg\">\n",
    "\n",
    "Similarity we can calculate for other two attributes(Humidity and Temp).\n",
    "\n",
    "Pick the highest gain attribute.\n",
    "\n",
    "<img src=\"img/7.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our root node is Outlook.\n",
    "\n",
    "<img src=\"img/8.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same thing for sub-trees till we get the tree.\n",
    "\n",
    "<img src=\"img/9.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we get the tree something like his.\n",
    "\n",
    "<img src=\"img/10.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with using the CART algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CART we use Gini index as a metric,\n",
    "\n",
    "We use the Gini Index as our cost function used to evaluate splits in the dataset.\n",
    "\n",
    "our target variable is Binary variable which means it take two values (Yes and No). There can be 4 combinations."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Actual=1 predicted 1\n",
    "1 0 , 0,1, 0 0\n",
    "P(Target=1).P(Target=1) + P(Target=1).P(Target=0) + \n",
    "P(Target=0).P(Target=1) + P(Target=0).P(Target=0) = 1\n",
    "\n",
    "P(Target=1).P(Target=0) + P(Target=0).P(Target=1) = 1 — P^2(Target=0) — P^2(Target=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini Index for Binary Target variable is\n",
    "\n",
    "= 1 — P^2(Target=0) — P^2(Target=1)\n",
    "\n",
    "\n",
    "A Gini score gives an idea of how good a split is by how mixed the classes are in the two groups created by the split. A perfect separation results in a Gini score of 0, whereas the worst case split that results in 50/50 classes.\n",
    "\n",
    "We calculate it for every row and split the data accordingly in our binary tree. We repeat this process recursively.\n",
    "\n",
    "For Binary Target variable, Max Gini Index value\n",
    "\n",
    "= 1 — (1/2)^2 — (1/2)^2\n",
    "\n",
    "= 1–2*(1/2)^2\n",
    "\n",
    "= 1- 2*(1/4)\n",
    "\n",
    "= 1–0.5\n",
    "\n",
    "= 0.5\n",
    "\n",
    "Similarly if Target Variable is categorical variable with multiple levels, the Gini Index will be still similar. If Target variable takes k different values, the Gini Index will be\n",
    "\n",
    "\n",
    "Maximum value of Gini Index could be when all target values are equally distributed.\n",
    "\n",
    "Similarly for Nominal variable with k level, the maximum value Gini Index is\n",
    "\n",
    "= 1–1/k\n",
    "\n",
    "Minimum value of Gini Index will be 0 when all observations belong to one label.\n",
    "\n",
    "Steps:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.compute the gini index for data-set\n",
    "2.for every attribute/feature:\n",
    "       1.calculate gini index for all categorical values\n",
    "       2.take average information entropy for the current attribute \n",
    "       3.calculate the gini gain\n",
    "3. pick the best gini gain attribute.\n",
    "4. Repeat until we get the tree we desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculations are similar to ID3 ,except the formula changes.\n",
    "\n",
    "for example :compute gini index for dataset\n",
    "\n",
    "<img src=\"img/11.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similarly we can follow other steps to build the tree\n",
    "\n",
    "<img src=\"img/12.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Code : Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine writing a program that has to predict if a picture contains a male or female. You would have to write tons of programming rules. If I’d give you another group of two images, you’d have to create new programming rules all over again. Machine Learning is a better way to solve these problems.\n",
    "\n",
    "Instead of programatically defining each rule, we use an algorithm that creates the rules for us. This type of algorithm is named a classifier. It takes data as input and shows a label as output.\n",
    "\n",
    "A practical example of this would be, given an image of a person, the classifier would predict if it’s female or male.\n",
    "\n",
    "The classifier has these steps:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.collect data\n",
    "2.train classifier\n",
    "3.make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we’ll use simple arrays as data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['woman']\n"
     ]
    }
   ],
   "source": [
    "#We import tree from sklearn and create the model\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()  \n",
    "\n",
    "#Then we create the training data for the classifier / decision tree:\n",
    "\n",
    "#[height, hair-length, voice-pitch]                                             \n",
    "X = [ [180, 15,0],                                                              \n",
    "      [167, 42,1],                                                              \n",
    "      [136, 35,1],                                                              \n",
    "      [174, 15,0],                                                              \n",
    "      [141, 28,1]]                                                              \n",
    "\n",
    "Y = ['man', 'woman', 'woman', 'man', 'woman']\n",
    "\n",
    "Y = ['man', 'woman', 'woman', 'man', 'woman']                                   \n",
    "\n",
    "clf = clf.fit(X, Y)                                                             \n",
    "prediction = clf.predict([[133, 37,1]])                                         \n",
    "print(prediction)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example : 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Used: https://archive.ics.uci.edu/ml/datasets/banknote+authentication\n",
    "        \n",
    "Check this link for details      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "\n",
    "#reading the dataset\n",
    "dataset = pd.read_csv(\"bill_authentication.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1372, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variance</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Curtosis</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.62160</td>\n",
       "      <td>8.6661</td>\n",
       "      <td>-2.8073</td>\n",
       "      <td>-0.44699</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.54590</td>\n",
       "      <td>8.1674</td>\n",
       "      <td>-2.4586</td>\n",
       "      <td>-1.46210</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.86600</td>\n",
       "      <td>-2.6383</td>\n",
       "      <td>1.9242</td>\n",
       "      <td>0.10645</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.45660</td>\n",
       "      <td>9.5228</td>\n",
       "      <td>-4.0112</td>\n",
       "      <td>-3.59440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32924</td>\n",
       "      <td>-4.4552</td>\n",
       "      <td>4.5718</td>\n",
       "      <td>-0.98880</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Variance  Skewness  Curtosis  Entropy  Class\n",
       "0   3.62160    8.6661   -2.8073 -0.44699      0\n",
       "1   4.54590    8.1674   -2.4586 -1.46210      0\n",
       "2   3.86600   -2.6383    1.9242  0.10645      0\n",
       "3   3.45660    9.5228   -4.0112 -3.59440      0\n",
       "4   0.32924   -4.4552    4.5718 -0.98880      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the Data\n",
    "\n",
    "we will divide our data into attributes and labels and will then divide the resultant data into both training and test sets. By doing this we can train our algorithm on one set of data and then test it out on a completely different set of data that the algorithm hasn't seen yet. This provides you with a more accurate view of how your trained algorithm will actually perform.\n",
    "\n",
    "To divide data into attributes and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.drop('Class', axis=1)  \n",
    "y = dataset['Class'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the X variable contains all the columns from the dataset, except the \"Class\" column, which is the label. The y variable contains the values from the \"Class\" column. The X variable is our attribute set and y variable contains corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, the test_size parameter specifies the ratio of the test set, which we use to split up 20% of the data in to the test set and 80% for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training and Making Predictions'\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "classifier = DecisionTreeClassifier()  \n",
    "classifier.fit(X_train, y_train)  \n",
    "\n",
    "y_pred = classifier.predict(X_test)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have trained our algorithm and made some predictions. Now we'll see how accurate our algorithm is. For classification tasks some commonly used metrics are confusion matrix, precision, recall, and F1 score. Lucky for us Scikit=-Learn's metrics library contains the classification_report and confusion_matrix methods that can be used to calculate these metrics for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[153   0]\n",
      " [  3 119]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       153\n",
      "          1       1.00      0.98      0.99       122\n",
      "\n",
      "avg / total       0.99      0.99      0.99       275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix  \n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the confusion matrix, you can see that out of 275 test instances, our algorithm misclassified only 4. This is 98.5 % accuracy. Not too bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth :\n",
    "# how deep the tree can be. The deeper the tree,\n",
    "# the more splits it has and it captures more information about the data. \n",
    "# We fit a decision tree with depths ranging from 1 to 32 and plot the training and test auc scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_samples_split:\n",
    "# represents the minimum number of samples required to split an internal node. \n",
    "# This can vary between considering at least one sample at each node to considering all of the samples at each node. \n",
    "# When we increase this parameter, the tree becomes more constrained as it has to consider more samples at each node.\n",
    "# Here we will vary the parameter from 10% to 100% of the samples\n",
    "\n",
    "# We can clearly see that when we consider 100% of the samples at each node, the model cannot learn enough about the data. \n",
    "# This is an underfitting case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
